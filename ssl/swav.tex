\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{verbatim}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Unsupervised Learning of Visual Features by Contrasting Cluster Assignments}

\author{\IEEEauthorblockN{Seri Lee}
\IEEEauthorblockA{\textit{Computer Science and Engineering} \\
\textit{Seoul National University}\\
Seoul, Republic of Korea\\
sally20921@snu.ac.kr} 
}

\maketitle

\begin{abstract}
Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods.
These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging.
In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. 
Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or "views") of the same image, instead of comparing features directly as in contrastive learning.
Simply put, we use a "swapped" prediction mechanism where we predict the code of a view from the representation of another view.
Our method can be trained with large and small batches and can scale to unlimited amounts of data. 
Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network.
In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements.
We validate our findings by achieving 75.3\% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.
\end{abstract}

\section{Introduction}
\cite{b1}.

Unsupervised visual representation learning, or self-supervised learning, aims at obtaining features without using manual annotations and is rapidly closing the performance gap with supervised pretraining in computer vision.
Many recent state-of-the-art methods build upon the instance discrimination task that considers each image of the dataset (or ``instance'') and its transformations as a separate class.
This task yields representations that are able to discriminate between different images, while achieving some invariance to image transformations.
Recent self-supervised methods that use instance discrimination rely on a combination of two elements: (1) a contrastive loss and (2) a set of image transformations.
The contrastive loss removes the notion of instance classes by directly comparing image features while the image transformations define the invariances encoded in the features.
Both elements are essential to the quality of the resulting networks and our work improves upon both the objective function and the transformations.

The contrastive loss explicitly compares pairs of image representations to push away representations from different images while pulling together those from transformations, or views, of the same image.
Since computing all the pairwise comparisons on a large dataset is not practical, most implementations approximate the loss by reducing the number of comparisons to random subsets of images during training.
An alternative to approximate the loss is the approximate the task-that is to relax the instance discrimination problem. 
For example, clustering-based methods discriminate between groups of images with similar features instead of individual images. 
The objective in clustering is tractable, but it does not scale well with the datasets as it requires a pass over the entire dataset to form image ``codes'' (\textit{i.e.}, cluster assignments) that are used as targets during training.
In this work, we use a different paradigm and propose to compute the codes online while enforcing consistency between codes obtained from views of the same image.
Comparing cluster assignments allows to contrast between different image views while not relying on explicit pairwise feature comparisons.
Specifically, we propose a simple ``swapped'' prediction problem where we predict the code of a view from teh representation of another view. 
We learn features by Swapping Assignments between multiple views of teh same image. The features and the codes are learned online, allowing our method to scale to potentially unlimited amounts of data.
In addition, SwAV works with small and large batch sizes and does not need a large memory bank or a momentum encoder.

Besides our online clustering method, we also propose an improvement to the image transformations. Most contrastive methods compare one pair of transformations per image, even though there is evidence that comparing more views during training improves the resulting model.
In this work, we propose multi-crop that uses smaller-sized images to increase the number of views while not increasing the memory or computational requirements during training.
We also observe that mapping small parts of a scene to more global views significantly boosts the performance.
Directly working with downsized images introduces a bias in the features, which can be avoided by using a mix of different sizes.
Our strategy is simple, yet effective, and can be applied to many self-supervised methods with consistent gain in performance.

We validate our contributions by evaluating our method on several standard self-supervised benchmarks. 
In particular, on the ImageNet linear evaluation protocol, we reach 75.3\% top-1 accuracy with a standard ResNet-50, and 78.5\% with a wider model. 
We also how that our multi-crop strategy is general, and improves the performance of different self-supervised methods, namely SimCLR, DeepCluster, and SeLa, between 2\% and 4\% top-1 accuracy on ImageNet. 
Overall, we amke the following contributions.

\begin{itemize}
    \item We propose a scalable online clustering loss that improves performance by +2\% on ImageNet and works in both large and small batch settings without a large memory bank or a momentum encoder.
    \item We introduce the multi-crop strategy to increase the number of views of an image with no computational or memory overhead. We observe a consistent improvements between 2\% and 4\% on ImageNet with this strategy on several self-supervised methods.
    \item Combining both technical contributions into a single model, we improve the performance of self-supervised by +4.2\% on ImageNet with a standard ResNet and outperforms supervised ImageNet pretraining on multiple downstream tasks.
    This is the first method to do so without finetuning the features, \textit{i.e.}, only with a linear classifier on top of frozen features.
\end{itemize}

\section{Related Work}
\textbf{Instance and contrastive learning}. Instance-level classification considers each image in a dataset as its own class. Dosovitskiy et al. assign a class explicitly to each image and learn a linear classifier with as many classes as images in the datasest.
As this approach becomes quickly intractable, Wu et al. mitigate this issue by replacing the classifier with a memory bank that stores previously-computed representations.
They rely on noise contrastive estimation to compare instances, which is a special form of contrastive learning. He et al. improve the training of contrastive methods by storing representations from a momentum encoder instead of the trained network.
More recently, Chen et al. show taht the memory bank can be entirely replaced with the elements from the same batch if the batch is large enough.
In contrast to this line of works, we avoid comparing every pair of iamges by mapping the image features to a set of trainable prototype vectors.

\textbf{Clustering for deep representation learning} Our work is also related to clustering-based methods. Caron et al. show taht k-means assignments can be used as pseudo-labels to learn visual representations.
This method scales to large uncurated dataset and can be used for pre-training of supervised networks. 
However, their formulation is not principled and recently, Asano et al. show how to cast the pseud-label assignment problem as an instance of the optimal transport problem.
We consider a similar formulation to map representations to prototype vectors, but we keep the soft assignment produced by the Sinkhorn-Knopp algorithm instead of approximating it into a hard assignment.
Besides, unlike Caron et al. and Asano et al., we obtain online assignments which allows our method to scale gracefully to any dataset size.

\textbf{Handcrafted pretext tasks}. Many self-supervised methods manipulate the input data to extract a supervised signal in the form of a pretext task.
We refer the reader to Jing et al. for an exhaustive and detailed review of this literature. Of particular interest, Misra and van der Maatan propose to encode the jugsaw puzzle task as an invariant for contrastive learning.
Jigsaw tiles are non-overlapping crops with small resolution that cover only part of the entire image area. In contrast, our multi-crop strategy consists in simply sampling multiple random crops with two different sizes: a standard size and a smaller one.

\section{Method}
Our goal is to learn visual features in an online fashion without supervision. To that effect, we propose an online clustering-based self-supervised method.
Typical clustering-based methods are offline in the sense that they alternate between a cluster assignment step where image features of the entire dataset are clustered, and a training step weher the cluster assignments, 
\textit{i.e.}, ``codes'' are predicted for different image views. 
Unfortunately, these moethods are not suitable for online learning as they require multiple passes over the dataset to compute the image features necessary for clustering.
In this section, we describe an alternative where we enforce consistency between codes from different augmentations of the same image.
This solution is inspired by contrastive instance learning as we do not consider the codes as target, but only enforce consistency mapping between views of the same image.
Our method can be interpreted as a way of contrasting between multiple image views by comparing their cluster assignments instead of their feature.

More precisely, we compute a code from an augmented version of the image and predict this code from otehr augmented versions of the same image.
Given two image features $z_t$ and $z_s$ from two different augmentations of the same image, we compute their codes $q_t$ and $q_s$, by matching these features to a set of $k$ prototypes $\{c_1, \ldots, c_K\}$.
WE then setup a ``swapped'' prediction problem with the following loss function:
\begin{equation}
    L(z_t, z_s) = l(z_t, q_s)+l(z_s, q_t)
\end{equation}
where the function $l(z,q)$ measures the fit between feature $z$ and and a code $q$, as detailed later.
Intuitively, our method compares the features $z_t$ and $z_s$ using the intermediate codes $q_t$ and $q_s$. 
If these two features capture the same information, it should be possible to predict the code from the other feature.
A similar comparison appears in contrastive learing where features are composed directly.

\subsection{Online clustering}
Each image $x_n$ is transformed into an augmented view $x_{nt}$ by applying a transformation $t$ sampled from the set of $T$ of image transformations.
The augmented view is mapped to a vector representation by applying a non-linear mapping $f_\theta$ to $x_{nt}$.
The feature is tehn projected to the unit sphere, \textit{i.e.}, $z_nt = f_\theta (x_{nt})/ \left\lVert f_\theta(x_{nt})\right\rVert_2$.
We then compute a code $q_{nt}$ from this feature by mapping $z_{nt}$ to a set of $K$ trainable prototypes vectors, ${c_1, \ldots, c_K}$. 
We denote by $C$ the matrix whose columns are the $c_1, \ldots, c_k$. We now describe how to compute these codes and update the prototypes online.

\subsection{Swapped prediction problem}

\bibliography{refs_contrastive}
\bibliographystyle{plain}

\end{document}