\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Unsupervised Domain Adaptation through Self-supervision}

\author{\IEEEauthorblockN{Seri Lee}
\IEEEauthorblockA{\textit{Computer Science and Engineering} \\
\textit{Seoul National University}\\
Seoul, Republic of Korea\\
sally20921@snu.ac.kr} 
}

\maketitle

\begin{abstract}
This paper addresses unsupervised domain adaptation \cite{b1}, the setting where labeled training data is available on a source domain, but the goal is to have good performance on a target domain with only unlabeled data.
Like much of previous work, we seek to align the learned representations of the source and target domains while preserving discriminability.
The way we accomplish alignment is by learning to perform auxiliary self-supervised task(s) on both domains simultaneously.
Each self-supervised task brings the two domains closer together along the direction relevant to that task.
Training this jointly with the main task classifier on the source domain is shown to successfully generalize to the unlabeled target domain.
The presented objective is straight-forward to implement and easy to optimize. We achieve state-of-the-art results on four out of seven standard benchmarks, and competitive results on segmentation adaptation.
We also demonstrate that our method composes well with another popular pixel-level adaptation method.
\end{abstract}

\section{Introduction}





\bibliography{refs_through}
\bibliographystyle{plain}

\end{document}
