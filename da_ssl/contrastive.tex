\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{verbatim}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Contrastive Adaptation Network for Unsupervised Domain Adaptation}

\author{\IEEEauthorblockN{Seri Lee}
\IEEEauthorblockA{\textit{Computer Science and Engineering} \\
\textit{Seoul National University}\\
Seoul, Republic of Korea\\
sally20921@snu.ac.kr} 
}

\maketitle

\begin{abstract}
Unsupervised Domain Adaptation (UDA) makes predictions for the target domain data while manual annotations are only available in the source domain.
Previous methods minimize the domain discrepancy neglecting the class information, which may lead to misalignment and poor generalization performance.
To address this issue, this paper proposes Contrastive Adaptation Network (CAN) optimizing a new metric which explicitly models the intra-class domain discrepancy and the inter-class domain discrepancy.
We design an alternating update strategy for training CAN in an end-to-end manner. Experiments on two real-world benchmarks Office-31 and VisDA-2017 demonstrate that CAN performs favorably against the state-of-the-art methods and produces more discriminative features.
\end{abstract}

\section{Introduction}
Recent advancements in deep neural networks have successfully improved a variety of learning problems \cite{b1}.
For supervised learning, however, massive labeled training data is still the key to learning an accurate deep model.
Although abundant labels may be available for a few pre-specific domains, such as ImageNet, manual labels often turn out to be difficult or expensive to obtain for every ad-hoc target domain or task.
The absence of in-domain labeled data hinders the application of data-fitting models in many real-world problems.

In the absence of labeled data from the target domain, Unsupervised Domain Adaptation (UDA) methods have emerged to mitigate the domain shift in data distributions.
It relates to unsupervised learning as it requires manual labels only from the source domain and zero labels from the target domain.
Among the recent work on UDA, a seminal line of work proposed by Long et al. aims at minimizing the discrepancy between the source and target domain in the deep neural network, where the domain discrepancy is measure by 
Maximum Mean Discrepancy (MMD) and Joint MMD (JMMD). MMD and JMMD have proven effective in many computer vision problems and demonstrated the state-of-the-art results on several UDA benchmarks.

Despite the success of previous methods based on MMD and JMMD, most of them measure the domain discrepancy at the \textit{domain} level, neglecting the class from which the samples are drawn.
These class-agnostic approaches, hence, do not discriminate whether samples from two domains should be aligned according to their class labels.
This can impair the adaptation performance due to the following reasons. First, samples of different classes may be aligned incorrectly, \textit{e.g.} both MMD and JMMd can be minimized even when the target-domain samples are misaligned with the source-domain samples of a different class.
Second, the learned decision boundary may generalize poorly for the target domain. There exist many sub-optimal solutions near the decision boundary. These solutions may overfit the source data well but are less discriminative for the target.

To address the above issues, we introduce a new \textit{Contrastive Domain Discrepancy} (CDD) objective to enable class-aware UDA.
We propose to minimize the intra-class discrepancy, \textit{i.e.} the domain discrepancy within the same class, and maximize the inter-class margin, \textit{i.e.} the domain discrepancy between different classes.
CDD will draw closer the source and target samples of the same underlying class, while pushing apart the samples from different classes.

Unfortunately, to estimate and optimize with CDD, we may not train a deep network out-of-the-box as we need to overcome the following technical issues.
First, we need labels from both domains to compute CDD, however, target labels are unknown in UDA. A straightforward way, of course, is to estimate the target labels by the network outputs during training.
However, because the estimation can be noisy, we find it can harm the adaptation performance.
Second, during mini-batch training, for a class $C$, the mini-batch may only contain samples from one domain (source or target), rendering it infeasible to estimate the intra-class domain discrepancy of $C$.
This can result in a less efficient adaptation. The above issues require special design of the network and the training paradigm.

In this paper, we propose \textit{Contrastive Adaptation Network} (CAN) to facilitate the optimization with CDD. During training, in addition to minimizing the cross-entropy loss on labeled source data, CAN alternatively estimates the underlying label hypothesis of target samples through clustering, and adapts the feature representations according to the CDD metric.
After clustering, the ambiguous target data and ambiguous classes are zeroed out in estimating the CDD.
Empirically we find that during training, an increasing amount of samples will be taken into account.
Such progressive learning can help CAN capture more accurate statistics of data distributions.
Moreover, to facilitate the mini-batch training of CAN, we employ the class-aware sampling for both source and target domains, \textit{i.e.} at each iteration, we sample data from both domains for each class within a random sampled class subset.
Class-aware sampling can improve the training efficiency and the adaptation performance.

We validate our method on two public UDA benchmarks: Office-31 and VisDA-2017. The experimental results show that our method performs favorably against the state-of-the-art UDA approaches, \textit{i.e.} we achieve the best-published result on the Office-31 benchmark and very competitive result on the challenging VisDA-2017 benchmark.
Ablation studies are presented to verity the contribution of each key component on our framework.
In a nutshell, our contributions are as follows,
\begin{itemize}
    \item We introduce a new discrepancy metric \textit{Contrastive Domain Discrepancy} (CDD) to perform class-aware alignment for unsupervised domain adaptation.
    \item We propose a network \textit{Contrastive Adaptation Network} to facilitate the end-to-end training with CDD.
\end{itemize}

\section{Related Work}
\textbf{Class-agnostic domain alignment}. A common practice for UDA is to minimize the discrepancy between domains to obtain domain-invariant features.
For example, Tzeng et al. proposed a kind of domain confusion loss to encourage the network to learn both semantically meaningful and domain invariant representations.
Long et al. proposed DAN and JAN to minimize the MMD and Joint MMD distance across domains respectively, over the domain-specific layers. Ganin et al. enabled the network to learn domain invariant representations in adversarial way by back-propagating the reverse gradients of the domain classifier.
Unlike these domain-discrepancy minimization methods, our method performs \textit{class-aware} domain alignment.

\textbf{Discriminative domain-invariant feature learning} Some previous works pay efforts to learn more discriminative features while performing domain alignment.
Adversarial Dropout Regularization (ADR) and Maximum Classifier Discrepancy (MCD) were proposed to train a deep neural network in adversarial way to avoid generating non-discriminative features lying in the region near the decision boundary.
Similar to us, Long et al. and Pei et al. take the class information into account while measuring the domain discrepancy. However, out method differs from theirs mainly in two aspects.
Firstly, we explicitly model two types of domain discrepancy, \textit{i.e.} the intra-class domain discrepancy and the inter-class domain discrepancy. The inter-class domain discrepancy, which has been ignored by most previous methods, is proved to be beneficial for enhancing the model adaptation performance.
Secondly, in the context of deep neural networks, we treat the training process as an alternative optimization over target label hypothesis and features.

\textbf{Intra-class compactness and inter-class separability modeling}. This paper is also related to the work that explicitly models the intra-class compactness and the inter-class separability, \textit{e.g.} the contrastive loss and the triplet loss.
These methods have been used in various applications, \textit{e.g.} face recognition, person re-identification, etc.
Different from these methods designed for a single domain, our work focuses on adaptation across domains.

\section{Methodology}
Unsupervised Domain Adaptation (UDA) aims at improving the model's generalization performance on target domain by mitigating the domain shift in data distribution of the source and target domain.
\begin{comment}
Formally, given a set of source domain samples, $ S=  \{(x^s_1. y^s_1), \ldots, (x^s__{N_s}, y^s_{N_s}) \}$, and target domain samples $T = \{ x^t_1, \ldots, x^t_{N_t} \}, x^s, x^t$ represent the input data, and $y^s \in \{  0,1, \ldots, M-1 \} $ denote the source data label of $M$ classes. 
The target data label $y^t \in \{0,1, \ldots, M-1\}$ is unknown. Thus, in UDA, we are interested in training network using labeled source domain data $S$ and unlabeled target domain data $T$ to make accurate predictions $\{\hat{y}^t\}$ on $T$. 
\end{comment}

We discuss our method in the context of deep neural networks. In deep neural networks, a sample owns hierarchical features/representations denoted by the activations of each layer $l \in L$. In the following, we use $\phi_l(x)$ to denote the outputs of layer $l$ in a deep neural network $\Phi_{\theta}$ for the input $x$,
where $\phi(\cdot)$ denotes the mapping defined by the deep neural network from the input to a specific layer.

\subsection{Maximum Mean Discrepancy Revisit}
In Maximum Mean Discrepancy (MMD), ${x^s_i}$ abd ${x^t_i}$ are sampled from the marginal distributions $P(X^s)$ and $Q(X^t)$ respectively.
Based on the observed samples, MMD performs a kernel two-sample test to determine whether to accept the null hypothesis $P=Q$ or not.
MMD is motivated by the fact that if two distributions are identical, all of their statistics should be the same.
Foramlly, MMD defines the difference between two distributions with their mean embeddings in the reproducing kernel Hilbert space (RKHS), \textit{i.e.}

\begin{equation}
    D_H(P,Q) \triangleq sup_{f~H}(\mathbb{E}_{X^s}-\mathbb{E}_{X^t}[f(X^t)])_H
\end{equation}
where $H$ is class of functions.

\subsection{Contrastive Adaptation Network}
Deep convolutional neural networks (CNNs)
\bibliography{refs_through}
\bibliographystyle{plain}

\end{document}
