\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{verbatim}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Transfer Adaptation Learning: A Decade Survery}

\author{\IEEEauthorblockN{Seri Lee}
\IEEEauthorblockA{\textit{Computer Science and Engineering} \\
\textit{Seoul National University}\\
Seoul, Republic of Korea\\
sally20921@snu.ac.kr}
ÃŸ
}

\maketitle

\begin{abstract}
The world we see is ever-changing and it always changes with people, things, and the environment. Domain is referred to as the state of the world at a certain moment. 
A research problem is characterized as transfer adaptation learning (TAL) when it needs knowledge correspondence between different moments/domains. 
Conventional machine learning aims to find a model with the minimum expected risk on test data by minimizing the regularized empirical risk on the training data, which however, supposes that the training and test data share similar joint probability distribution.
TAL aims to build models that can perform tasks of target domain by learning knowledge from a semantic related but distribution different source domain.
It is an energetic research filled with increasing influence and importance, which is presenting a blowout publication trend. This paper surveys the advances of TAL methodologies in the past decade, and the technical challenges and essential problems of TAL have been observed and discussed with deep insights and new perspectives.
Broader solutions of transfer adaptation learning is being created by researchers, for instance, re-weighting adaptation, feature adaptation, classifier adaptation, deep network adaptation and adversarial adaptation
\end{abstract}

\begin{IEEEkeywords}
Domain adaptation, fine-grained, subdomain
\end{IEEEkeywords}

\section{Introduction}
In recent years, deep learning methods have achieved impressive success in computer vision \cite{b1}, which, however usually needs large amounts of labeled data to train a good deep network.
In the real world, it is often expensive and laborsome to collect enough labeled data. For a target task with the shortage of labeled data, there is a strong motivation to build effective learners that can leverage rich labeled data from related source domain.
However, this learning paradigm suffers from the shift of data distributions across different domains, which will undermine the generalization ability of machine learning models.

Learning a discriminative model in the presence of the shift between the training and test data distributions is known as domain adaptation or transfer learning. Previous shallow domain adaptation methods bridge the source and target domains by learning invariant feature representations or estimate instance importance without using target labels.
Recent studies have shown that deep networks can learn more transferable features for domain adaptation, by disentangling explanatory factors of variations behind domains.
The latest advantages have been achieved by embedding domain adaptation modules in the pipeline of deep feature learning to extract domain-invariant representations.

The previous deep domain adaptation methods, mainly learn a global domain shift, i.e., aligning the global source and target distributions without considering the relationships between two subdomains in both domains (a subdomain contains the samples within the same class).
As a result, not only all the data from the source and target domains will be confused, but also the discriminative structures can be mixed up.
This might lose the fine-grained information for each category. 

After global domain adaptation, the distributions of the two domains are approximately the same, but the data in different subdomains are too close to be classified accurately.
This is a common problem in previous global domain adaptation methods. Hence, matching the global source and target domains may not work well for diverse scenarios.

With regard to the challenge of global domain shift, recently, more and more researchers pay attention to subdomain adaptation (also called semantic alignment or matching conditional distribution) which is centered on learning a local domain shift, i.e., accurately aligning the distribution of the relevant subdomains within the same category in the source and target domains.
After subdomain adaptation, with the local distribution that is approximately the same, the global distribution is also approximately the same.
However, all of them are adversarial methods that contain several loss functions and converge slowly. 

Based on the subdomain adaptation, we propose a deep subdomain adaptation network (DSAN) to align the relevant subdomain distributions of activations in multiple domain-specific layers across domains for unsupervised domain adaptation.
DSAN extends the feature representation ability of deep adaptation networks (DANs) by aligning relevant subdomain distributions as mentioned earlier.
A key improvement over previous domain adaptation methods is the capability of subdomain adaptation to capture the fine-grained information for each category, which can be trained in an end-to-end framework.
To enable proper alignment, we design a local maximum mean discrepancy (LMMD), which measures teh Hilbert-Schmidt norm between kernel mean embedding of empirical distributions of the relevant subdomains in source and target domains with considering the weight of different samples.

The LMMD method can be achieved with most feedforward network models and can be trained efficiently using standard backpropagation.
In addition, our DSAN is very simple and easy to implement. Note that the most remarkable results are achieved by adversarial methods recently. 
Experiments show that DSAN, which is a nonadversarial method, can obtain remarkable results for standard domain adaptation on both object recognition tasks and digit classification tasks.

\section{Related Work}
\subsection{Domain Adaptation} Recent years have witnessed many approaches to solve the visual domain adaptation problem, which is also commonly framed as the visual data set bias problem.
Previous shallow methods for domain adaptation include reweighting the training data so that they can more closely reflect those in the test distribution, and finding a transformation in a lower dimensional manifold that draws the source and target subspaces closer.

Recent studies have shown that deep networks can learn more transferable features for domain adaptation, by disentangling explanatory factors of variations behind domains.
The latest advances have been achieved by embedding domain adaptation modules in the pipeline of deep feature learning to extract domain-invariant representations.
Two main approaches are identified among the literature. The first is statistic moment matching-based approach, i.e., MMD, central moment discrepancy (CMD), and second-order statistics matching.
The seoncd commonly used approach is based on an adversarial loss, which encourage samples from different domains to be nondiscriminative with respect to domain labels, i.e., domain adversarial net-based adaptation methods borrowing the idea of GAN.
Generally, the adversarial approaches can achieve better performance than the statistic moment matching-based approaches. In addition, most state-of-the-art approaches are domain adversarial net-based adaptation methods.
Our DSAN is an MMD-based method. We show that DSAN without adversarial loss can achieve remarkable results.

\subsection{Maximum Mean Discrepancy}
MMD has been adopted in many approaches, for domain adaptation.
In addition, there are some extensions of MMD. Conditional MMD and joint MMD measure the Hilbert-Schmidt norm between kernel mean embedding of empirical conditional and joint distributions of the source and target data, respectively.
Weighted MMD alleviates the class weight bias by assigning class-specific weights to source data.
However, our local MMD measures the discrepancy between kernel mean embedding relevant subdomains in source and target domains with considering the weight of different samples.

\subsection{Subdomain Adaptation} 
Recently, we have witnessed considerable interest and research for subdomain adaptation that focuses on accurately aligning the distributions of the relevant subdomains.
Multiadversarial domain adaptation (MADA) captures the multimode structures to enable fine-grained alignment of different data distributions based on multiple-domain discriminator.
Moving semantic transfer network (MSTN) learns the semantic representations for unlabeled target samples by aligning labeled source centroid and pseudolabeled target centroid.
CDAN conditions the adversarial adaptation models on discriminative information conveyed in the classifier predictions.
Co-DA constructs multiple diverse feature spaces and aligns source and target distributions in each of them individually while encouraging that alignments agree with each other with regard to the class predictions on the unlabeled target examples.
The adversarial loss is adopted by all of them. 

\section{Deep Subdomain Adaptation Network}
To divide the source and target domains into multiple subdomains that contain the samples within the same class,
the relationships between the samples should be exploited. 
It is well known that the samples within the same category are more relevant.
However, data in the target domain is unlabeled. Hence, we would use the output of the networks as the pseudolabels of target domain data, which will be detailed later.


\bibliography{refs}
\bibliographystyle{plain}


\end{document}